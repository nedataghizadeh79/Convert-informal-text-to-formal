{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <div dir=\"rtl\">\n",
    "    در این نسخه از pos tag shuffler می‌خواهیم از یک مدل زبانی n-gram استفاده کنیم برای یافتن بهترین ترکیب pos tag ها\n",
    "    بدین شکل که تمامی جایگشت‌های ممکن از pos tag ها را می‌سازیم و با استفاده از این مدل زبانی به آن‌ها امتیاز می‌دهیم سپس بهترین ها را به مدل زبانی اصلی می‌دهیم.\n",
    "    </div>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import POSTagger, Normalizer\n",
    "from enum import Enum, auto\n",
    "from hazm import SentenceTokenizer, WordTokenizer\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "tagger = POSTagger(model=\"resources/pos_tagger.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_n_gram_model(n, resource='resources/Datasets/MirasText_sample.txt'):\n",
    "    with open(resource, 'r') as f:\n",
    "        text = f.read()\n",
    "    sentences = SentenceTokenizer().tokenize(text)\n",
    "    tri_grams = []\n",
    "    for sentece in tqdm(sentences):\n",
    "        words = WordTokenizer().tokenize(sentece)\n",
    "        tagged_words = tagger.tag(words)\n",
    "\n",
    "        # make 3-gram from tags\n",
    "        tags = ['START'] + [tag for word, tag in tagged_words] + ['END']\n",
    "        ng = ngrams(tags, n)\n",
    "        tri_grams.extend(ng)\n",
    "    return tri_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_dic(n):\n",
    "    probability_dic = {}\n",
    "    ngram_model = make_n_gram_model(n)\n",
    "    for tup in ngram_model:\n",
    "        probability_dic[tup] = probability_dic.get(tup, 0.01) + 1\n",
    "        if tup[0] == 'START':\n",
    "            probability_dic[tup] += 100\n",
    "    return probability_dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from pickle file!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "dataset_action = \"load\"\n",
    "\n",
    "if dataset_action == \"build\":\n",
    "    probability_dic = get_probability_dic(n)\n",
    "    dataset_action = \"save\"\n",
    "\n",
    "if dataset_action == \"save\" and probability_dic:\n",
    "    with open(\"resources/Datasets/ngram_shuffler.pickle\", \"wb\") as f:\n",
    "        pickle.dump(probability_dic, f)\n",
    "        print(\"Saved dataset to pickle file!\")\n",
    "elif dataset_action == \"load\":\n",
    "    with open(\"resources/Datasets/ngram_shuffler.pickle\", \"rb\") as f:\n",
    "        probability_dic = pickle.load(f)\n",
    "        print(\"Loaded dataset from pickle file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def calculate_probability_of_postags(n, postags: list):\n",
    "        probability = 1\n",
    "        tags = ['START'] + postags + ['END']\n",
    "        for i in range(len(tags) - (n-1)):\n",
    "            probability *= probability_dic.get(tuple(tags[i:i+n]), 1)\n",
    "        return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_permutations(n, sentence: str):\n",
    "    words = WordTokenizer().tokenize(sentence)\n",
    "    tagged_words = tagger.tag(words)\n",
    "    print(tagged_words)\n",
    "    permutations = itertools.permutations(tagged_words)\n",
    "    all_possibilities = [(permutation, calculate_probability_of_postags(n, [tag for word, tag in permutation])) for permutation in permutations]\n",
    "    return sorted(all_possibilities, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('علی', 'NOUN'), ('با', 'ADP'), ('من', 'PRON'), ('حرف', 'NOUN'), ('زد', 'VERB'), ('.', 'PUNCT')]\n",
      "3.2016573093341353e+22\n"
     ]
    }
   ],
   "source": [
    "text = \"علی با من حرف زد.\"\n",
    "tagged_text = tagger.tag(WordTokenizer().tokenize(text))\n",
    "print(tagged_text)\n",
    "print(calculate_probability_of_postags(n, [tag for word, tag in tagged_text]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('من', 'PRON'), ('و', 'CCONJ'), ('علی', 'NOUN'), ('رفتیم', 'VERB'), ('به', 'ADP'), ('مدرسه', 'NOUN'), ('.', 'PUNCT')]\n",
      "من به علی و مدرسه رفتیم .\n",
      "من به مدرسه و علی رفتیم .\n",
      "من و علی به مدرسه رفتیم .\n",
      "من و مدرسه به علی رفتیم .\n",
      "و من به علی مدرسه رفتیم .\n",
      "و من به مدرسه علی رفتیم .\n",
      "و علی به من مدرسه رفتیم .\n",
      "و مدرسه به من علی رفتیم .\n",
      "من و به علی مدرسه رفتیم .\n",
      "من و به مدرسه علی رفتیم .\n"
     ]
    }
   ],
   "source": [
    "permutations = get_possible_permutations(n, \"من و علی رفتیم به مدرسه.\")\n",
    "for permutation in permutations:\n",
    "    print(' '.join([word for word, tag in permutation[0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datastuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
