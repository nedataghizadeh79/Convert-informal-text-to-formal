{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <div dir=\"rtl\">\n",
    "        این نوتبوک، روی دیتاست میراث جملات را بررسی می‌کند و ترتیب اجزاء جمله را بسته به POS tag شان یاد می‌گیرد.\n",
    "        از این طریق می‌تواند بهترین جایگشت‌های ممکن برای کلمات را پیدا کند.\n",
    "        <br>\n",
    "        هر جمله‌ای که به مدل داده می‌شود، ابتدا tokenize می‌شود سپس pos tag هر توکن پیدا می‌شود.\n",
    "        ساختار جمله از طریق این pos tag ها شناسایی می‌شود.\n",
    "        سپس دنبال این ساختار شناسایی شده در داده‌های یادگیری (که از میراث استفاده شده) می‌گردد تا بهترین جایگشت این توکن‌ها را پیدا کند.\n",
    "        تمام جایگشت‌های ممکن که احتمال خوبی برای درست بودن دارند به مدل‌های زبانی داده می‌شوند و به آن‌ها امتیاز تعلق می‌گیرد.\n",
    "    </div>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from hazm) (4.3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from hazm) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from hazm) (1.24.3)\n",
      "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from hazm) (0.9.9)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from hazm) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.3.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.65.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/datastuff/lib/python3.9/site-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import POSTagger, Normalizer\n",
    "from enum import Enum, auto\n",
    "from hazm import SentenceTokenizer, WordTokenizer\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "tagger = POSTagger(model=\"resources/pos_tagger.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## تنظیمات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_permutations = 30\n",
    "min_acceptable_score_ngram = 10\n",
    "min_acceptable_score_bert = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('رفته', 'VERB'), ('بودم', 'VERB'), ('خونه\\u200cی', 'NOUN,EZ'), ('دوستم', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"رفته بودم خونه ی دوستم\"\n",
    "\n",
    "normalized_text = Normalizer().normalize(text)\n",
    "tagged_text = tagger.tag(normalized_text.split())\n",
    "print(tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "each sentence will be associated with a vector of len(Tag) elements, each element representing the number of words with that tag in the sentence\n",
    "(N, Ne, V, Aj, Adv, Det, P, Conj, Pron, Num, Postp, Prep, Interj, Abbrev, Aux, Punc)\n",
    "example: بابا آب داد\n",
    "(2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 ,0 ,0 ,0 ,0, 0, 0, 0, 0)\n",
    "\"\"\"\n",
    "\n",
    "class Tag(Enum):\n",
    "    NOUN = auto()\n",
    "    NE = auto()\n",
    "    VERB = auto()\n",
    "    ADJ = auto()\n",
    "    ADV = auto()\n",
    "    ADP = auto()\n",
    "    DET = auto()\n",
    "    P = auto()\n",
    "    CONJ = auto()\n",
    "    PRON = auto()\n",
    "    NUM = auto()\n",
    "    POSTP = auto()\n",
    "    PREP = auto()\n",
    "    INTJ = auto()\n",
    "    ABBREV = auto()\n",
    "    AUX = auto()\n",
    "    PUNCT = auto()\n",
    "    SCONJ = auto()\n",
    "    CCONJ = auto()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    dataset = {\n",
    "        (1,0,2,...): {\n",
    "            (N,V,N,...): 2,\n",
    "            (N,N,V,...): 1,\n",
    "        }, ...\n",
    "    }\n",
    "\"\"\"\n",
    "def tag_dataset(resource='resources/Datasets/MirasText_sample.txt'):\n",
    "    with open(resource, 'r') as f:\n",
    "        text = f.read()\n",
    "    sentences = SentenceTokenizer().tokenize(text)\n",
    "    dataset = {}\n",
    "    for sentece in tqdm(sentences):\n",
    "        words = WordTokenizer().tokenize(sentece)\n",
    "        tagged_words = tagger.tag(words)\n",
    "\n",
    "        sentence_vector = [0] * len(Tag)\n",
    "        for word, tag in tagged_words:\n",
    "            pure_tag = tag.split(',')[0] # some tags are followed by ,EZ so we ignore it!\n",
    "            tag_index = Tag[pure_tag].value - 1\n",
    "            sentence_vector[tag_index] += 1\n",
    "\n",
    "        sentence_parts_order = tuple([tag.split(',')[0] for _, tag in tagged_words])\n",
    "\n",
    "        if tuple(sentence_vector) in dataset:\n",
    "            if sentence_parts_order in dataset[tuple(sentence_vector)]:\n",
    "                dataset[tuple(sentence_vector)][sentence_parts_order] += 1\n",
    "            else:\n",
    "                dataset[tuple(sentence_vector)][sentence_parts_order] = 1\n",
    "        else:\n",
    "            dataset[tuple(sentence_vector)] = {sentence_parts_order: 1}\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <div dir=\"rtl\">\n",
    "        برای این که فرایند ساخت tagged_dataset زمان بر است، از pickle کردن آن استفاده می‌کنیم که بتوانیم سریعا از روی فایل داده‌های train شده را بازیابی کنیم.\n",
    "    </div>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from pickle file!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "dataset_action = \"load\"\n",
    "\n",
    "if dataset_action == \"build\":\n",
    "    tagged_dataset = tag_dataset()\n",
    "    dataset_action = \"save\"\n",
    "\n",
    "if dataset_action == \"save\" and tagged_dataset:\n",
    "    with open(\"resources/Datasets/trained_postag_orders.pickle\", \"wb\") as f:\n",
    "        pickle.dump(tagged_dataset, f)\n",
    "        print(\"Saved dataset to pickle file!\")\n",
    "elif dataset_action == \"load\":\n",
    "    with open(\"resources/Datasets/trained_postag_orders.pickle\", \"rb\") as f:\n",
    "        tagged_dataset = pickle.load(f)\n",
    "        print(\"Loaded dataset from pickle file!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# hamshahri_dataset = tag_dataset(resource='resources/Datasets/Hamshahri-Corpus.txt')\n",
    "# with open(\"Datasets/hamshahri_trained_postag_orders.pickle\", \"wb\") as f:\n",
    "#         pickle.dump(tagged_dataset, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <div dir=\"rtl\">\n",
    "        کاری که دوست داریم انجام بدهیم، این است که بتوانیم جملات را بر اساس اجزاء جمله‌شان دسته‌بندی کنیم.\n",
    "        اما اگر بخواهیم به صورت دقیق تمام اجزاء را در نظر بگیریم کمی دشوار خواهد بود و به دیتاست خیلی بزرگ‌تری نیاز خواهیم داشت.\n",
    "        برای همین باید بتوانیم کمی دسته‌بندی‌ها را جامع‌تر کنیم تا دسته‌های مختلف را در بر بگیرد.\n",
    "        <br>\n",
    "        برای حل این مشکل، چندین راه وجود دارد. یکی این که که \n",
    "    </div>\n",
    "</html>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <div dir=\"rtl\">\n",
    "    حال می‌خواهیم برای هر جمله، بهترین چینش‌ها را انتخاب کنیم. برای این کار باید POS tag های جمله را شناسایی کرده و به دنبال جایگشت‌های مطلوب آن در دیتاست ساخته شده بگردیم.\n",
    "    </div>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_permutations(sentence: str):\n",
    "    words = WordTokenizer().tokenize(sentence)\n",
    "    tagged_words = tagger.tag(words)\n",
    "    permutations = []\n",
    "    print(tagged_words)\n",
    "    sentence_vector = [0] * len(Tag)\n",
    "    for word, tag in tagged_words:\n",
    "        pure_tag = tag.split(',')[0] # some tags are followed by ,EZ so we ignore it!\n",
    "        tag_index = Tag[pure_tag].value - 1\n",
    "        sentence_vector[tag_index] += 1\n",
    "\n",
    "    vector_additions = [[0]*len(sentence_vector)] + [[0]*i + [1] + [0]*(len(sentence_vector)-i-1) for i in range(len(sentence_vector))]\n",
    "    for addition, vector_addition in enumerate(vector_additions):\n",
    "        vector = [x + y for x, y in zip(sentence_vector, vector_addition)]\n",
    "        if tuple(vector) in tagged_dataset:\n",
    "            proper_sentence_orders = tagged_dataset[tuple(vector)]\n",
    "            sorted_sentence_orders = sorted(proper_sentence_orders.items(), key=lambda item: item[1], reverse=True)\n",
    "            \n",
    "            # build all permutations of the sentence\n",
    "            all_permutations = list(itertools.permutations(tagged_words))\n",
    "            for proper_order, _ in sorted_sentence_orders:\n",
    "                if addition > 0:\n",
    "                    the_order = tuple(filter(lambda x: x != Tag(addition).name , proper_order))\n",
    "                else:\n",
    "                    the_order = proper_order\n",
    "                permutations += filter(lambda x: tuple([i[1].split(',')[0] for i in x]) == the_order, all_permutations)\n",
    "                if len(permutations) >= max_number_of_permutations:\n",
    "                    break\n",
    "    return permutations[:max_number_of_permutations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('علی', 'NOUN'), ('به', 'ADP'), ('من', 'PRON'), ('گفت', 'VERB')]\n",
      "من به علی گفت\n",
      "به من علی گفت\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"علی به من گفت\"\n",
    "possible_permutations = get_possible_permutations(sample_text)\n",
    "\n",
    "for cp in possible_permutations:\n",
    "    print(' '.join([i[0] for i in cp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20076/20076 [00:00<00:00, 2079542.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for _ in tqdm(tagged_dataset.keys()):\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('در', 'ADP'), ('حال', 'NOUN,EZ'), ('ایران', 'NOUN'), ('.', 'PUNCT'), ('جمعیت', 'NOUN'), ('است', 'VERB'), ('کاهش', 'NOUN')]\n",
      "حال ایران در جمعیت کاهش است .\n",
      "حال ایران در کاهش جمعیت است .\n",
      "حال جمعیت در ایران کاهش است .\n",
      "حال جمعیت در کاهش ایران است .\n",
      "حال کاهش در ایران جمعیت است .\n",
      "حال کاهش در جمعیت ایران است .\n",
      "ایران حال در جمعیت کاهش است .\n",
      "ایران حال در کاهش جمعیت است .\n",
      "ایران جمعیت در حال کاهش است .\n",
      "ایران جمعیت در کاهش حال است .\n",
      "ایران کاهش در حال جمعیت است .\n",
      "ایران کاهش در جمعیت حال است .\n",
      "جمعیت حال در ایران کاهش است .\n",
      "جمعیت حال در کاهش ایران است .\n",
      "جمعیت ایران در حال کاهش است .\n",
      "جمعیت ایران در کاهش حال است .\n",
      "جمعیت کاهش در حال ایران است .\n",
      "جمعیت کاهش در ایران حال است .\n",
      "کاهش حال در ایران جمعیت است .\n",
      "کاهش حال در جمعیت ایران است .\n",
      "کاهش ایران در حال جمعیت است .\n",
      "کاهش ایران در جمعیت حال است .\n",
      "کاهش جمعیت در حال ایران است .\n",
      "کاهش جمعیت در ایران حال است .\n",
      "حال در ایران جمعیت کاهش است .\n",
      "حال در ایران کاهش جمعیت است .\n",
      "حال در جمعیت ایران کاهش است .\n",
      "حال در جمعیت کاهش ایران است .\n",
      "حال در کاهش ایران جمعیت است .\n",
      "حال در کاهش جمعیت ایران است .\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"در حال ایران . جمعیت است کاهش\"\n",
    "possible_permutations = get_possible_permutations(sample_text)\n",
    "\n",
    "for cp in possible_permutations:\n",
    "    print(' '.join([i[0] for i in cp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('به', 'ADP'), ('مدرسه', 'NOUN'), ('رفتم', 'VERB'), ('من', 'PRON'), ('.', 'PUNCT')]\n",
      "من به مدرسه رفتم .\n",
      "به من مدرسه رفتم .\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"به مدرسه رفتم من.\"\n",
    "possible_permutations = get_possible_permutations(sample_text)\n",
    "\n",
    "for cp in possible_permutations:\n",
    "    print(' '.join([i[0] for i in cp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('رفتم', 'VERB'), ('به', 'ADP'), ('دانشگاه', 'NOUN'), ('.', 'PUNCT')]\n",
      "به دانشگاه رفتم .\n",
      "دانشگاه به رفتم .\n",
      "به دانشگاه رفتم .\n",
      "به دانشگاه رفتم .\n",
      "به دانشگاه رفتم .\n",
      "به دانشگاه رفتم .\n",
      "به دانشگاه رفتم .\n",
      "به دانشگاه رفتم .\n",
      "به دانشگاه رفتم .\n",
      "به دانشگاه رفتم .\n",
      "دانشگاه به رفتم .\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"رفتم به دانشگاه.\"\n",
    "possible_permutations = get_possible_permutations(sample_text)\n",
    "\n",
    "for cp in possible_permutations:\n",
    "    print(' '.join([i[0] for i in cp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datastuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
