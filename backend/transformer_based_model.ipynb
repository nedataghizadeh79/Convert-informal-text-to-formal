{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, TFAutoModel,\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "model = TFAutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc350035",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ما در هوشواره معتقدیم با انتقال صحیح دانش و آگاهی، همه افراد می‌توانند از ابزارهای هوشمند استفاده کنند. شعار ما هوش مصنوعی برای همه است.\"\n",
    "tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cfeaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ما در هوشواره معتقدیم با انتقال صحیح دانش و آگاهی، همه افراد می‌توانند از ابزارهای هوشمند استفاده کنند. شعار ما هوش مصنوعی برای همه است.\"\n",
    "encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13873db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "input_ids = tf.convert_to_tensor(encoded_input[\"input_ids\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d20be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = tf.convert_to_tensor(encoded_input[\"attention_mask\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dbb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type_ids = tf.convert_to_tensor(encoded_input[\"token_type_ids\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca22543",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f88ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "token_embeddings = output.last_hidden_state\n",
    "for i in range(token_embeddings.shape[1]):\n",
    "    token_id = encoded_input[\"input_ids\"][0][i].numpy()\n",
    "    token = tokenizer.convert_ids_to_tokens(np.array([token_id]))\n",
    "    embedding = token_embeddings[0][i].numpy()\n",
    "    print(f\"Token: {token}\")\n",
    "    print(f\"Embedding: {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbd5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "\n",
    "# Prepare the training data\n",
    "train_sentences = [\"این یک مثال است\", \"مثال دیگری است\"]\n",
    "train_labels = [\n",
    "    [\"DET\", \"NUM\", \"NOUN\", \"VERB\", \"PUNCT\"],\n",
    "    [\"NOUN\", \"ADJ\", \"VERB\", \"DET\", \"ADJ\", \"NOUN\", \"PUNCT\"],\n",
    "]\n",
    "\n",
    "train_tokenized = tokenizer(train_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "train_input_ids = train_tokenized[\"input_ids\"].numpy()\n",
    "train_attention_mask = train_tokenized[\"attention_mask\"]\n",
    "train_labels_encoded = np.array([[tokenizer.convert_tokens_to_ids(label) for label in labels] for labels in train_labels])\n",
    "\n",
    "# Define the model architecture\n",
    "num_labels = len(tokenizer.get_vocab())\n",
    "model = TFBertForTokenClassification.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Trainthe model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "model.fit(\n",
    "    x=(train_input_ids, train_attention_mask),\n",
    "    y=train_labels_encoded,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "test_sentence = \"این یک مثال دیگر است\"\n",
    "test_tokenized = tokenizer(test_sentence, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "test_input_ids = test_tokenized[\"input_ids\"]\n",
    "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
    "test_logits = model.predict((test_input_ids, test_attention_mask))[0]\n",
    "test_labels = tf.argmax(test_logits, axis=-1).numpy()\n",
    "test_tokens = tokenizer.convert_ids_to_tokens(test_input_ids[0])\n",
    "test_reordered_tokens = [test_tokens[i] for i in test_labels[1:-1]]\n",
    "test_reordered_sentence = tokenizer.convert_tokens_to_string(test_reordered_tokens)\n",
    "print(test_reordered_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
