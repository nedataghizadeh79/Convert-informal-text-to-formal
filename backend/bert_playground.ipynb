{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b253dbb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, TFAutoModel, TFBertForTokenClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "model = TFAutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc350035",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ما در هوشواره معتقدیم با انتقال صحیح دانش و آگاهی، همه افراد می‌توانند از ابزارهای هوشمند استفاده کنند. شعار ما هوش مصنوعی برای همه است.\"\n",
    "# tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00cfeaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "541b91d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13873db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.convert_to_tensor(encoded_input[\"input_ids\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7d20be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = tf.convert_to_tensor(encoded_input[\"attention_mask\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05dbb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type_ids = tf.convert_to_tensor(encoded_input[\"token_type_ids\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ca22543",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65f88ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c915fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_embeddings = output.last_hidden_state\n",
    "# for i in range(token_embeddings.shape[1]):\n",
    "#     token_id = encoded_input[\"input_ids\"][0][i].numpy()\n",
    "#     token = tokenizer.convert_ids_to_tokens(np.array([token_id]))\n",
    "#     embedding = token_embeddings[0][i].numpy()\n",
    "#     print(f\"Token: {token}\")\n",
    "#     print(f\"Embedding: {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55fbd5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_14724\\2004771841.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_labels_encoded = np.array([[tokenizer.convert_tokens_to_ids(label) for label in labels] for labels in train_labels])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TFBertForTokenClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Define the model architecture\u001b[39;00m\n\u001b[0;32m     19\u001b[0m num_labels \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mget_vocab())\n\u001b[1;32m---> 20\u001b[0m model \u001b[39m=\u001b[39m TFBertForTokenClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mHooshvareLab/bert-base-parsbert-uncased\u001b[39m\u001b[39m\"\u001b[39m, num_labels\u001b[39m=\u001b[39mnum_labels)\n\u001b[0;32m     22\u001b[0m \u001b[39m# Trainthe model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m5e-5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TFBertForTokenClassification' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare the training data\n",
    "train_sentences = [\"این یک مثال است\", \"مثال دیگری است\"]\n",
    "train_labels = [\n",
    "    [\"DET\", \"NUM\", \"NOUN\", \"VERB\", \"PUNCT\"],\n",
    "    [\"NOUN\", \"ADJ\", \"VERB\", \"DET\", \"ADJ\", \"NOUN\", \"PUNCT\"],\n",
    "]\n",
    "\n",
    "train_tokenized = tokenizer(train_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "train_input_ids = train_tokenized[\"input_ids\"].numpy()\n",
    "train_attention_mask = train_tokenized[\"attention_mask\"]\n",
    "train_labels_encoded = np.array([[tokenizer.convert_tokens_to_ids(label) for label in labels] for labels in train_labels])\n",
    "\n",
    "# Define the model architecture\n",
    "num_labels = len(tokenizer.get_vocab())\n",
    "token_class_model = TFBertForTokenClassification.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Trainthe model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "token_class_model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "token_class_model.fit(\n",
    "    x=(train_input_ids, train_attention_mask),\n",
    "    y=train_labels_encoded,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "test_sentence = \"این یک مثال دیگر است\"\n",
    "test_tokenized = tokenizer(test_sentence, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "test_input_ids = test_tokenized[\"input_ids\"]\n",
    "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
    "test_logits = token_class_model.predict((test_input_ids, test_attention_mask))[0]\n",
    "test_labels = tf.argmax(test_logits, axis=-1).numpy()\n",
    "test_tokens = tokenizer.convert_ids_to_tokens(test_input_ids[0])\n",
    "test_reordered_tokens = [test_tokens[i] for i in test_labels[1:-1]]\n",
    "test_reordered_sentence = tokenizer.convert_tokens_to_string(test_reordered_tokens)\n",
    "print(test_reordered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da2d1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
    "import random\n",
    "\n",
    "# Load the pre-trained ParsBERT model and tokenizer\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ec2de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input sentence\n",
    "text = \"ای دوست بیا تا\"\n",
    "\n",
    "# Tokenize the input sentence and replace a random token with the [UNK] token\n",
    "tokenized_text = tokenizer.encode(text, add_special_tokens=True)\n",
    "unk_token_index = random.randint(1, len(tokenized_text) - 2)  # Choose a random token to replace\n",
    "tokenized_text[unk_token_index] = tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9cb20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Convert the tokenized sentence to a tensor and run the model to obtain the probability distribution\n",
    "input_ids = tf.constant(tokenized_text)[None, :]  # Batch size 1\n",
    "outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c966d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.nn.softmax(outputs.logits[0, unk_token_index], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94cff839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 'ای' (0.24)\n",
      "2. 'با' (0.03)\n",
      "3. 'سلام' (0.03)\n",
      "4. 'بیا' (0.03)\n",
      "5. 'اقای' (0.02)\n",
      "6. 'به' (0.02)\n",
      "7. 'یه' (0.02)\n",
      "8. 'یک' (0.02)\n",
      "9. 'خانهی' (0.02)\n",
      "10. 'خانم' (0.02)\n"
     ]
    }
   ],
   "source": [
    "# Get the top 5 most likely filled-in tokens and their probabilities\n",
    "k = 10\n",
    "top_k = tf.math.top_k(predictions, k=k)\n",
    "for i, token_index in enumerate(top_k.indices.numpy()):\n",
    "    token = tokenizer.decode([token_index])\n",
    "    score = top_k.values.numpy()[i]\n",
    "    print(f\"{i+1}. '{token}' ({score:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9325f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Load the pre-trained ParsBERT model and tokenizer for sequence classification\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2f9bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'من چقدر خوشحالم',\n",
    "    'لعنت به این شانس',\n",
    "]\n",
    "\n",
    "# Tokenize the input sentences and convert them to a tensor\n",
    "max_length = 128  # Maximum sequence length for the model\n",
    "encoded_inputs = tokenizer(sentences, padding=True, truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
    "input_ids = encoded_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1a7665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model to obtain the probability distribution over the classes\n",
    "outputs = model(input_ids)\n",
    "probabilities = tf.nn.softmax(outputs.logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "372f29e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: 'من چقدر خوشحالم'\n",
      "Probability: 0.4683\n",
      "\n",
      "Sentence 2: 'لعنت به این شانس'\n",
      "Probability: 0.4526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the probabilities for each input sentence\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Sentence {i+1}: '{sentence}'\")\n",
    "    print(f\"Probability: {probabilities[i, 1]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88a25f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "\n",
    "# Load the pre-trained ParsBERT model and tokenizer\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ddedd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"من به مدرسه می‌روم.\",\n",
    "    \"می‌روم به مدرسه من.\",\n",
    "    'من می‌روم به مدرسه.',\n",
    "    'به مدرسه من می‌روم.',\n",
    "    'به مدرسه می‌روم من.',\n",
    "    'می‌روم من به مدرسه.',\n",
    "]\n",
    "\n",
    "# Tokenize the input sentences and convert them to a tensor\n",
    "max_length = 128\n",
    "encoded_inputs = tokenizer(sentences, padding=True, truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
    "input_ids = encoded_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f9fdddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Compute the log-likelihood score of each sentence\n",
    "log_likelihoods = []\n",
    "for i in range(len(sentences)):\n",
    "    outputs = model(input_ids[i][None, :])\n",
    "    log_probabilities = tf.nn.log_softmax(outputs.logits[0], axis=-1)\n",
    "    is_special_token = tf.math.logical_or(tf.math.equal(input_ids[i], tokenizer.cls_token_id),\n",
    "                                          tf.math.logical_or(tf.math.equal(input_ids[i], tokenizer.sep_token_id),\n",
    "                                                             tf.math.equal(input_ids[i], tokenizer.pad_token_id)))\n",
    "    token_log_probabilities = tf.boolean_mask(log_probabilities, tf.math.logical_not(is_special_token))\n",
    "    sentence_log_likelihood = tf.reduce_sum(token_log_probabilities).numpy()\n",
    "    log_likelihoods.append(sentence_log_likelihood)\n",
    "\n",
    "# Convert log-likelihood scores to probabilities using the softmax function\n",
    "probabilities = tf.nn.softmax(log_likelihoods).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "477d3b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: 'من به مدرسه می‌روم.'\n",
      "Probability: 0.0000\n",
      "\n",
      "Sentence 2: 'می‌روم به مدرسه من.'\n",
      "Probability: 0.0000\n",
      "\n",
      "Sentence 3: 'من می‌روم به مدرسه.'\n",
      "Probability: 0.0000\n",
      "\n",
      "Sentence 4: 'به مدرسه من می‌روم.'\n",
      "Probability: 0.0000\n",
      "\n",
      "Sentence 5: 'به مدرسه می‌روم من.'\n",
      "Probability: 1.0000\n",
      "\n",
      "Sentence 6: 'می‌روم من به مدرسه.'\n",
      "Probability: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    print(f\"Sentence {i+1}: '{sentences[i]}'\")\n",
    "    print(f\"Probability: {probabilities[i]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f221c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: 'من به مدرسه می‌روم.'\n",
      "Token: [CLS]\n",
      "Top-5 predictions: ، و. من دارم\n",
      "\n",
      "Token: من\n",
      "Top-5 predictions: من ما دارم بنده الان\n",
      "\n",
      "Token: به\n",
      "Top-5 predictions: به گاهی هم من همیشه\n",
      "\n",
      "Token: مدرسه\n",
      "Top-5 predictions: مدرسه خانه مدارس دبستان باشگاه\n",
      "\n",
      "Token: میروم\n",
      "Top-5 predictions: میروم میرود میرویم میروند نمیروم\n",
      "\n",
      "Token: .\n",
      "Top-5 predictions: . : ؟! -\n",
      "\n",
      "Token: [SEP]\n",
      "Top-5 predictions: . : ؟ کد!\n",
      "\n",
      "Sentence 2: 'می‌روم به مدرسه من.'\n",
      "Token: [CLS]\n",
      "Top-5 predictions: . میروم ، مدرسه و\n",
      "\n",
      "Token: میروم\n",
      "Top-5 predictions: میروم میرود میرویم بروید بروم\n",
      "\n",
      "Token: به\n",
      "Top-5 predictions: به در سمت پیش یه\n",
      "\n",
      "Token: مدرسه\n",
      "Top-5 predictions: مدرسه خانه کلاس مدرسهی دبیرستان\n",
      "\n",
      "Token: من\n",
      "Top-5 predictions: من ما شما او تو\n",
      "\n",
      "Token: .\n",
      "Top-5 predictions: .! ؟ : -\n",
      "\n",
      "Token: [SEP]\n",
      "Top-5 predictions: . : ؟! یک\n",
      "\n",
      "Sentence 3: 'من می‌روم به مدرسه.'\n",
      "Token: [CLS]\n",
      "Top-5 predictions: ،. میروم و مدرسه\n",
      "\n",
      "Token: من\n",
      "Top-5 predictions: من ما بعد دارم الان\n",
      "\n",
      "Token: میروم\n",
      "Top-5 predictions: میروم میرم میرود میرویم بروم\n",
      "\n",
      "Token: به\n",
      "Top-5 predictions: به یه الى یک در\n",
      "\n",
      "Token: مدرسه\n",
      "Top-5 predictions: مدرسه خانه مدارس باشگاه بیرون\n",
      "\n",
      "Token: .\n",
      "Top-5 predictions: . :! ؟ -\n",
      "\n",
      "Token: [SEP]\n",
      "Top-5 predictions: . : ؟ کد!\n",
      "\n",
      "Sentence 4: 'به مدرسه من می‌روم.'\n",
      "Token: [CLS]\n",
      "Top-5 predictions: ،. میروم من مدرسه\n",
      "\n",
      "Token: به\n",
      "Top-5 predictions: به در گاهی امروز از\n",
      "\n",
      "Token: مدرسه\n",
      "Top-5 predictions: مدرسه خانه کلاس مدرسهای دبیرستان\n",
      "\n",
      "Token: من\n",
      "Top-5 predictions: من ما شما او انها\n",
      "\n",
      "Token: میروم\n",
      "Top-5 predictions: میروم میرود میرویم میروند میروی\n",
      "\n",
      "Token: .\n",
      "Top-5 predictions: . : ؟! -\n",
      "\n",
      "Token: [SEP]\n",
      "Top-5 predictions: . : ؟! کد\n",
      "\n",
      "Sentence 5: 'به مدرسه می‌روم من.'\n",
      "Token: [CLS]\n",
      "Top-5 predictions: میروم. ، من بیا\n",
      "\n",
      "Token: به\n",
      "Top-5 predictions: به من وقتی دارم گاهی\n",
      "\n",
      "Token: مدرسه\n",
      "Top-5 predictions: مدرسه خانه دبیرستان دبستان مدارس\n",
      "\n",
      "Token: میروم\n",
      "Top-5 predictions: میروم میرود میروند میرویم میروی\n",
      "\n",
      "Token: من\n",
      "Top-5 predictions: من … شما ما.\n",
      "\n",
      "Token: .\n",
      "Top-5 predictions: . : ؟! -\n",
      "\n",
      "Token: [SEP]\n",
      "Top-5 predictions: . : ؟ یک!\n",
      "\n",
      "Sentence 6: 'می‌روم من به مدرسه.'\n",
      "Token: [CLS]\n",
      "Top-5 predictions: میروم ،. میروند میرود\n",
      "\n",
      "Token: میروم\n",
      "Top-5 predictions: میروم میرود میروند میرویم بروم\n",
      "\n",
      "Token: من\n",
      "Top-5 predictions: من ما و تو شما\n",
      "\n",
      "Token: به\n",
      "Top-5 predictions: به الى در سمت تا\n",
      "\n",
      "Token: مدرسه\n",
      "Top-5 predictions: مدرسه خانه مدارس بیرون باشگاه\n",
      "\n",
      "Token: .\n",
      "Top-5 predictions: .! ؟ : -\n",
      "\n",
      "Token: [SEP]\n",
      "Top-5 predictions: . : ؟! یک\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top-k predicted tokens for each token in the input sentence\n",
    "k = 5\n",
    "for i in range(len(sentences)):\n",
    "    print(f\"Sentence {i+1}: '{sentences[i]}'\")\n",
    "    inputs = tf.constant(input_ids[i][None, :])\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits[0]\n",
    "    top_k = tf.math.top_k(logits, k=k)\n",
    "    for j in range(len(encoded_inputs['input_ids'][i])):\n",
    "        print(f\"Token: {tokenizer.decode([encoded_inputs['input_ids'][i][j]])}\")\n",
    "        print(f\"Top-{k} predictions: {tokenizer.decode(top_k.indices[j])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c205e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed34dd5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'tf_bert_for_masked_lm_3' (type TFBertForMaskedLM).\n\nData of type <class 'torch.Tensor'> is not allowed only (<class 'tensorflow.python.framework.ops.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>, <class 'keras.engine.keras_tensor.KerasTensor'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_bert_for_masked_lm_3' (type TFBertForMaskedLM):\n  • input_ids=tensor([[    2,  2078,  2031,  5000, 14182,    15,     4]])\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m log_likelihoods \u001b[39m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(sentences)):\n\u001b[1;32m---> 16\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids[i][\u001b[39mNone\u001b[39;49;00m, :])\n\u001b[0;32m     17\u001b[0m     log_probabilities \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(outputs\u001b[39m.\u001b[39mlogits[\u001b[39m0\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m     is_special_token \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlogical_or(torch\u001b[39m.\u001b[39meq(input_ids[i], tokenizer\u001b[39m.\u001b[39mcls_token_id),\n\u001b[0;32m     19\u001b[0m                                          torch\u001b[39m.\u001b[39mlogical_or(torch\u001b[39m.\u001b[39meq(input_ids[i], tokenizer\u001b[39m.\u001b[39msep_token_id),\n\u001b[0;32m     20\u001b[0m                                                           torch\u001b[39m.\u001b[39meq(input_ids[i], tokenizer\u001b[39m.\u001b[39mpad_token_id)))\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\modeling_tf_utils.py:433\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m--> 433\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_args_and_kwargs)\n\u001b[0;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\modeling_tf_utils.py:563\u001b[0m, in \u001b[0;36minput_processing\u001b[1;34m(func, config, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m         output[main_input_name] \u001b[39m=\u001b[39m main_input\n\u001b[0;32m    562\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 563\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    564\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(main_input)\u001b[39m}\u001b[39;00m\u001b[39m is not allowed only \u001b[39m\u001b[39m{\u001b[39;00mallowed_types\u001b[39m}\u001b[39;00m\u001b[39m is accepted for\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mmain_input_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m         )\n\u001b[0;32m    568\u001b[0m \u001b[39m# Populates any unspecified argument with their default value, according to the signature.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m parameter_names:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'tf_bert_for_masked_lm_3' (type TFBertForMaskedLM).\n\nData of type <class 'torch.Tensor'> is not allowed only (<class 'tensorflow.python.framework.ops.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>, <class 'keras.engine.keras_tensor.KerasTensor'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_bert_for_masked_lm_3' (type TFBertForMaskedLM):\n  • input_ids=tensor([[    2,  2078,  2031,  5000, 14182,    15,     4]])\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"من به مدرسه میروم.\",\n",
    "    \"من میروم به مدرسه.\",\n",
    "    \"به مدرسه من میروم.\",\n",
    "    \"میروم من به مدرسه.\"\n",
    "]\n",
    "\n",
    "# Tokenize the input sentences and convert them to a tensor\n",
    "max_length = 128\n",
    "encoded_inputs = tokenizer(sentences, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "input_ids = encoded_inputs[\"input_ids\"]\n",
    "\n",
    "# Compute the log-likelihood score of each sentence\n",
    "log_likelihoods = []\n",
    "for i in range(len(sentences)):\n",
    "    outputs = model(input_ids[i][None, :])\n",
    "    log_probabilities = torch.nn.functional.log_softmax(outputs.logits[0], dim=-1)\n",
    "    is_special_token = torch.logical_or(torch.eq(input_ids[i], tokenizer.cls_token_id),\n",
    "                                         torch.logical_or(torch.eq(input_ids[i], tokenizer.sep_token_id),\n",
    "                                                          torch.eq(input_ids[i], tokenizer.pad_token_id)))\n",
    "    token_log_probabilities = torch.masked_select(log_probabilities, torch.logical_not(is_special_token))\n",
    "    sentence_log_likelihood = torch.sum(token_log_probabilities).item()\n",
    "    log_likelihoods.append(sentence_log_likelihood)\n",
    "\n",
    "# Sort the sentences based on their log-likelihood scores\n",
    "sorted_sentences = [sentences[i] for i in torch.argsort(torch.tensor(log_likelihoods), descending=True).tolist()]\n",
    "\n",
    "# Print the sorted sentences\n",
    "print(\"Sorted sentences:\")\n",
    "for sentence in sorted_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db02a193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Load the ParsBERT model\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Define the input sentence\n",
    "input_sentence = \"این یک جمله تست است\"\n",
    "\n",
    "# Tokenize the input sentence\n",
    "tokens = tokenizer(input_sentence, return_tensors=\"tf\")[\"input_ids\"]\n",
    "\n",
    "# Generate all possible permutations of tokens\n",
    "permutations = list(itertools.permutations(tokens.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e97ceb7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1] = 2042 is not in [0, 1) [Op:GatherV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     masked_sequence \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat([tf\u001b[39m.\u001b[39mones_like(tokens) \u001b[39m*\u001b[39m tokenizer\u001b[39m.\u001b[39mmask_token_id, sequence], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m     probabilities \u001b[39m=\u001b[39m model(masked_sequence)[\u001b[39m0\u001b[39m][:, \u001b[39m0\u001b[39m, :]\n\u001b[1;32m----> 7\u001b[0m     score \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mreduce_prod(tf\u001b[39m.\u001b[39;49mgather(probabilities, tokens[\u001b[39m0\u001b[39;49m]), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m     scores\u001b[39m.\u001b[39mappend(score)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Sort the permutations by their scores\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7261\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7262\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1] = 2042 is not in [0, 1) [Op:GatherV2]"
     ]
    }
   ],
   "source": [
    "# Calculate the probability of each permutation\n",
    "scores = []\n",
    "for permutation in permutations:\n",
    "    sequence = tf.constant(permutation, shape=(1, len(permutation)))\n",
    "    masked_sequence = tf.concat([tf.ones_like(tokens) * tokenizer.mask_token_id, sequence], axis=1)\n",
    "    probabilities = model(masked_sequence)[0][:, 0, :]\n",
    "    score = tf.math.reduce_prod(tf.gather(probabilities, tokens[0]), axis=0).numpy()[0]\n",
    "    scores.append(score)\n",
    "\n",
    "# Sort the permutations by their scores\n",
    "results = sorted(zip(scores, permutations), reverse=True)\n",
    "\n",
    "# Print the results\n",
    "for score, permutation in results:\n",
    "    sentence = tokenizer.decode(permutation)\n",
    "    print(f\"Score: {score:.5f} Sentence: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a513f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordering 1: من\n",
      "Score: [0.55159485 0.44840518]\n",
      "Ordering 2: پردازش\n",
      "Score: [0.5159384  0.48406163]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained ParsBERT model for sequence classification\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62817b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input sentences with different orderings\n",
    "orderings = [\n",
    "    \"من\",\n",
    "    \"پردازش\",\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "input_ids = tokenizer(orderings, padding=True, truncation=True, return_tensors=\"tf\").input_ids\n",
    "\n",
    "# Get scores from the model\n",
    "logits = model(input_ids).logits\n",
    "\n",
    "# Compute the probabilities using softmax\n",
    "probs = tf.nn.softmax(logits, axis=1)\n",
    "\n",
    "# Print the scores for each sentence\n",
    "for i, ordering in enumerate(orderings):\n",
    "    print(f\"Ordering {i + 1}: {ordering}\")\n",
    "    print(f\"Score: {probs[i].numpy()}\")\n",
    "\n",
    "# Find the most correct ordering\n",
    "# most_correct_idx = tf.argmax(probs, axis=0).numpy()\n",
    "# print(f\"\\nMost correct ordering: {orderings[most_correct_idx]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "493caae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.39822698, 0.1911104 ],\n",
       "       [0.31495565, 0.2511805 ]], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "23af939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `TFBertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "All model checkpoint layers were used when initializing TFBertLMHeadModel.\n",
      "\n",
      "All the layers of TFBertLMHeadModel were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertLMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids)\n\u001b[0;32m     17\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m---> 18\u001b[0m probability_score \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39;49mloss)\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSentence \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mprobability_score\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'NoneType'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "37a07e2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFBertLMHeadModel' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m input_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant([encoded_sentence])\n\u001b[0;32m     10\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids)\n\u001b[1;32m---> 11\u001b[0m model\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     12\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(encoded_sentence, outputs\u001b[39m.\u001b[39mloss)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TFBertLMHeadModel' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "# Define the sentence permutations\n",
    "sentences = [\"من به مدرسه می‌روم\", \"به مدرسه می‌روم من\"]\n",
    "\n",
    "# Encode the sentences into tokens\n",
    "encoded_sentences = [tokenizer.encode(s) for s in sentences]\n",
    "\n",
    "# Calculate the probability scores for each sentence permutation\n",
    "for i, encoded_sentence in enumerate(encoded_sentences):\n",
    "    input_ids = tf.constant([encoded_sentence])\n",
    "    outputs = model(input_ids)\n",
    "    loss = outputs.loss\n",
    "    print(encoded_sentence, outputs.loss)\n",
    "    probability_score = tf.exp(-loss)\n",
    "    print(f\"Sentence {i + 1}: {probability_score.numpy()[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "90a4d869",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Define your model architecture\u001b[39;00m\n\u001b[0;32m      5\u001b[0m input_seq \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(\u001b[39m128\u001b[39m,))\n\u001b[1;32m----> 6\u001b[0m x \u001b[39m=\u001b[39m LSTM(\u001b[39m128\u001b[39;49m)(input_seq)\n\u001b[0;32m      7\u001b[0m x \u001b[39m=\u001b[39m Dense(vocab_size, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)(x)\n\u001b[0;32m      8\u001b[0m model \u001b[39m=\u001b[39m Model(input_seq, x)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py:556\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m inputs, initial_state, constants \u001b[39m=\u001b[39m rnn_utils\u001b[39m.\u001b[39mstandardize_args(\n\u001b[0;32m    552\u001b[0m     inputs, initial_state, constants, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_constants\n\u001b[0;32m    553\u001b[0m )\n\u001b[0;32m    555\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 556\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    558\u001b[0m \u001b[39m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[39m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[39m# input_spec to include them.\u001b[39;00m\n\u001b[0;32m    562\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\input_spec.py:235\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    233\u001b[0m     ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[1;32m--> 235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    237\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m, found ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m         )\n\u001b[0;32m    241\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define your model architecture\n",
    "input_seq = Input(shape=(128,))\n",
    "x = LSTM(128)(input_seq)\n",
    "x = Dense(vocab_size, activation='softmax')(x)\n",
    "model = Model(input_seq, x)\n",
    "\n",
    "# Compile your model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train your model\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=32, epochs=10)\n",
    "\n",
    "# Evaluate your model\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('Test score:', score)\n",
    "\n",
    "# Generate probabilities for sentence permutations\n",
    "permutations = ['من به مدرسه میروم', 'به مدرسه میروم من']\n",
    "for perm in permutations:\n",
    "    words = perm.split()\n",
    "    probs = []\n",
    "    for i in range(len(words)):\n",
    "        seq = tokenizer.texts_to_sequences([words[:i+1]])\n",
    "        seq = pad_sequences(seq, maxlen=max_len, padding='pre')\n",
    "        prob = model.predict(seq)[0][-1]\n",
    "        probs.append(prob)\n",
    "    sentence_prob = np.prod(probs)\n",
    "    print(perm, sentence_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2ce89346",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = [\"پسر خردسال در حیاط با گربه بازی می‌کند\"]\n",
    "cand = [\"پسر بچه در حیاط با یک گربه دارد بازی می‌کند\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2ec01b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mHooshvareLab/bert-base-parsbert-uncased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mHooshvareLab/bert-base-parsbert-uncased\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m ref_encodings \u001b[39m=\u001b[39m tokenizer(ref, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m      7\u001b[0m cand_encodings \u001b[39m=\u001b[39m tokenizer(cand, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:467\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    466\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 467\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    468\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    469\u001b[0m     )\n\u001b[0;32m    470\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    471\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    472\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\modeling_utils.py:2432\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2417\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2418\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2419\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m   2420\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcache_dir\u001b[39m\u001b[39m\"\u001b[39m: cache_dir,\n\u001b[0;32m   2421\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mforce_download\u001b[39m\u001b[39m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2430\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m: commit_hash,\n\u001b[0;32m   2431\u001b[0m     }\n\u001b[1;32m-> 2432\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs)\n\u001b[0;32m   2434\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2435\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2436\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   2437\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    414\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[0;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    118\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1361\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[0;32m   1362\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[1;32m-> 1364\u001b[0m     http_get(\n\u001b[0;32m   1365\u001b[0m         url_to_download,\n\u001b[0;32m   1366\u001b[0m         temp_file,\n\u001b[0;32m   1367\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1368\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1369\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1370\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[0;32m   1371\u001b[0m     )\n\u001b[0;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[0;32m    531\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    534\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    535\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    539\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    540\u001b[0m )\n\u001b[1;32m--> 541\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[0;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    543\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\models.py:760\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m'\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    759\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 760\u001b[0m         \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    761\u001b[0m             \u001b[39myield\u001b[39;00m chunk\n\u001b[0;32m    762\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\http\\client.py:454\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    452\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    453\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 454\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    455\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\http\\client.py:498\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    493\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    495\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 498\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    499\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    500\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "\n",
    "ref_encodings = tokenizer(ref, padding=True, truncation=True, max_length=128)\n",
    "cand_encodings = tokenizer(cand, padding=True, truncation=True, max_length=128)\n",
    "\n",
    "ref_tensors = {key: torch.tensor(val) for key, val in ref_encodings.items()}\n",
    "cand_tensors = {key: torch.tensor(val) for key, val in cand_encodings.items()}\n",
    "\n",
    "ref_outputs = model(**ref_tensors)\n",
    "cand_outputs = model(**cand_tensors)\n",
    "\n",
    "ref_embeddings = ref_outputs.last_hidden_state[:, 0, :]\n",
    "cand_embeddings = cand_outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb045ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "ref_sentences = [[sent] for sent in ref]\n",
    "cand_sentence = [sent for sent in cand]\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(cand_sentence, ref_sentences)\n",
    "print(\"BLEU score:\", bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "828bb772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS]\n",
      "Probabilities: [4.4809287e-05 1.6985367e-05 5.1590276e-05 2.0360555e-04 3.2640313e-04\n",
      " 2.5878873e-01 3.2047261e-03 1.0163234e-01 1.2900239e-02 3.8944837e-01\n",
      " 1.7244516e-01 6.0937028e-02]\n",
      "Token: [MASK]\n",
      "Probabilities: [7.7144883e-05 6.0363291e-05 8.5927575e-05 1.6650952e-04 5.0698168e-04\n",
      " 2.2023416e-01 2.9697830e-02 1.2470782e-01 1.7611835e-02 3.2659408e-01\n",
      " 6.5764397e-02 2.1449293e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [7.04608465e-05 6.42156228e-05 7.42365446e-05 1.63019446e-04\n",
      " 6.09883748e-04 2.82955229e-01 2.65347492e-02 1.14709921e-01\n",
      " 1.29546113e-02 3.36161941e-01 1.06002465e-01 1.19699247e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [5.7504280e-05 4.4545246e-05 8.8364184e-05 1.6260204e-04 5.5753108e-04\n",
      " 3.5771888e-01 2.3241306e-02 6.7435980e-02 1.2004697e-02 2.8813416e-01\n",
      " 1.4786080e-01 1.0269360e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [4.6513425e-05 3.6335132e-05 8.1242637e-05 1.3376902e-04 4.3901056e-04\n",
      " 3.6924934e-01 1.7306471e-02 4.8218898e-02 1.1280631e-02 2.2637163e-01\n",
      " 2.0911914e-01 1.1771704e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [3.77544384e-05 2.86897030e-05 7.09793967e-05 1.08917564e-04\n",
      " 3.69622663e-04 3.63320947e-01 1.51108736e-02 4.16993462e-02\n",
      " 1.13624716e-02 1.89832911e-01 2.50677466e-01 1.27380058e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [3.3090262e-05 2.3581451e-05 6.2092418e-05 9.4525902e-05 3.1693798e-04\n",
      " 3.6054051e-01 1.4624194e-02 3.7647031e-02 1.1308662e-02 1.6633850e-01\n",
      " 2.7758813e-01 1.3142274e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [3.0301357e-05 2.0386600e-05 5.5492364e-05 7.9695907e-05 2.6011863e-04\n",
      " 3.5316551e-01 1.5719935e-02 3.5180315e-02 1.3221430e-02 1.7245650e-01\n",
      " 2.7732429e-01 1.3248605e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [3.1573447e-05 1.9347546e-05 4.4620403e-05 6.6784276e-05 2.4275699e-04\n",
      " 3.1253350e-01 1.8033881e-02 5.1139124e-02 1.6037846e-02 2.2002847e-01\n",
      " 2.5553131e-01 1.2629083e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [3.8664875e-05 1.8378982e-05 2.7844777e-05 4.7753947e-05 2.5377172e-04\n",
      " 2.3965228e-01 1.8720031e-02 1.7667486e-01 1.4510721e-02 2.2560810e-01\n",
      " 2.0393604e-01 1.2051155e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [2.8816783e-05 1.2285783e-05 1.1737507e-05 2.4300740e-05 2.0960528e-04\n",
      " 1.5308771e-01 1.9167762e-02 5.3293520e-01 7.7137495e-03 9.2226073e-02\n",
      " 1.0406655e-01 9.0516180e-02]\n",
      "Token: [MASK]\n",
      "Probabilities: [1.74346627e-04 8.42729860e-05 4.16470584e-06 5.17476510e-05\n",
      " 1.49476982e-04 1.02107786e-01 7.85055906e-02 6.82178326e-03\n",
      " 1.51302099e-01 4.73852642e-02 2.54823029e-01 3.58590513e-01]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForMaskedLM, BertTokenizer\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = TFBertForMaskedLM.from_pretrained('HooshvareLab/bert-base-parsbert-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('HooshvareLab/bert-base-parsbert-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "76adc796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS]\n",
      "Probabilities: [3.5259771e-05 1.9251916e-05 2.1564220e-04 9.8753713e-05 2.0632017e-04\n",
      " 9.9942482e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [3.0996930e-04 2.6423519e-04 1.2717439e-03 5.6489184e-04 1.5339991e-03\n",
      " 9.9605513e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [1.8119441e-04 1.8839908e-04 9.4285549e-04 4.2124020e-04 1.5720628e-03\n",
      " 9.9669433e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [1.0453192e-04 8.9604750e-05 6.4787071e-04 2.3261992e-04 1.0475052e-03\n",
      " 9.9787784e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [7.0846130e-05 3.9859427e-05 1.8473469e-04 9.9069090e-05 5.3867843e-04\n",
      " 9.9906689e-01]\n",
      "Token: [MASK]\n",
      "Probabilities: [1.5545944e-03 8.8933198e-04 3.8920178e-05 2.2776834e-04 5.3936901e-04\n",
      " 9.9675012e-01]\n"
     ]
    }
   ],
   "source": [
    "# Define the input sentence\n",
    "sentence = \"سلام من به مدرسه رفتم.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "\n",
    "# Replace each token with [MASK] and tokenize again\n",
    "masked_token_indices = []\n",
    "for i, token in enumerate(tokenized_sentence):\n",
    "    if token not in ['[CLS]', '[SEP]']:\n",
    "        masked_token_indices.append(i)\n",
    "        tokenized_sentence[i] = '[MASK]'\n",
    "        \n",
    "input_ids = tokenizer.encode(tokenized_sentence, return_tensors='tf')\n",
    "\n",
    "# Use the model to predict the masked tokens\n",
    "outputs = model(input_ids)\n",
    "predictions = outputs.logits\n",
    "masked_predictions = tf.gather(predictions[0], masked_token_indices, axis=1)\n",
    "\n",
    "# Calculate the probability distribution for each masked token\n",
    "softmax_fn = tf.keras.layers.Softmax()\n",
    "probabilities = softmax_fn(masked_predictions)\n",
    "\n",
    "# Print out the results\n",
    "for i, token_index in enumerate(masked_token_indices):\n",
    "    token = tokenizer.convert_ids_to_tokens([input_ids[0][token_index].numpy()])[0]\n",
    "    print(f\"Token: {token}\")\n",
    "    print(f\"Probabilities: {probabilities[i].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "894baa6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 6), dtype=float32, numpy=\n",
       "array([[-6.8146734 , -7.4198055 , -5.0037956 , -5.784787  , -5.0479865 ,\n",
       "         3.437519  ],\n",
       "       [-7.1932235 , -7.352857  , -5.781553  , -6.593063  , -5.5940638 ,\n",
       "         0.88186073],\n",
       "       [-8.633314  , -8.594322  , -6.9839716 , -7.7896814 , -6.4727407 ,\n",
       "        -0.02068512],\n",
       "       [-8.856813  , -9.010898  , -7.032614  , -8.0569    , -6.552139  ,\n",
       "         0.30708072],\n",
       "       [-8.530693  , -9.1058445 , -7.572283  , -8.195386  , -6.5020847 ,\n",
       "         1.0233738 ],\n",
       "       [-1.9143311 , -2.47283   , -5.601788  , -3.8349717 , -2.9729009 ,\n",
       "         4.5489545 ],\n",
       "       [-5.8808074 , -5.4052696 , -6.227822  , -5.6999063 , -3.674399  ,\n",
       "        12.559     ],\n",
       "       [-3.1178765 , -4.7900357 , -6.8098793 , -1.4210021 , -1.4866896 ,\n",
       "         4.5351663 ]], dtype=float32)>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fbb51211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['انجا']\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fa5a928b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام خوبی ؟\n",
      "1 / 6\n",
      "سلام ؟ خوبی\n",
      "2 / 6\n",
      "خوبی سلام ؟\n",
      "3 / 6\n",
      "خوبی ؟ سلام\n",
      "4 / 6\n",
      "؟ سلام خوبی\n",
      "5 / 6\n",
      "؟ خوبی سلام\n",
      "6 / 6\n",
      "('سلام خوبی ؟', 10790.347008575605)\n",
      "('خوبی سلام ؟', 4307.226293668622)\n",
      "('؟ خوبی سلام', 25.940788701154002)\n",
      "('خوبی ؟ سلام', 6.993595142937502)\n",
      "('؟ سلام خوبی', 4.644772194668399)\n",
      "('سلام ؟ خوبی', 3.1370356348312494)\n"
     ]
    }
   ],
   "source": [
    "def calculate_sentence_score(sentence_tokens):\n",
    "    sentence_score = 1\n",
    "    sentence = ' '.join(sentence_tokens)\n",
    "    for i,token in enumerate(sentence_tokens):\n",
    "        found = False\n",
    "        masked_text = ' '.join(tokens[:i])+' [MASK] '+' '.join(tokens[i+1:])\n",
    "\n",
    "        inputs = tokenizer(masked_text, return_tensors=\"tf\")\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        mask_token_index = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
    "        # mask_token_index = i\n",
    "        mask_token_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "        k = 1000\n",
    "\n",
    "        top_k_values, top_k_indices = tf.math.top_k(mask_token_logits, k=k)\n",
    "        predicted_token_indices = top_k_indices.numpy()\n",
    "        probabilities = tf.nn.softmax(top_k_values).numpy()\n",
    "        predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_indices)\n",
    "\n",
    "        for j,predicted_token in enumerate(predicted_tokens):\n",
    "            if predicted_token == token:\n",
    "                sentence_score *= probabilities[j]\n",
    "                # print('found',sentence_score)\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        epsilon = 0.00001\n",
    "\n",
    "        if not found:\n",
    "            sentence_score *= epsilon\n",
    "            # print('not found',sentence_score)\n",
    "\n",
    "        # print(token)\n",
    "        # for i in range(k):\n",
    "        #     print(predicted_tokens[i],probabilities[i])\n",
    "        # print('----')\n",
    "\n",
    "    soft_parameter = 1000000000000\n",
    "\n",
    "    return sentence_score*soft_parameter\n",
    "\n",
    "\n",
    "import itertools\n",
    "text = \"سلام خوبی؟\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "permutations = list(itertools.permutations(tokens))\n",
    "scores = []\n",
    "for i,permutation in enumerate(permutations):\n",
    "    print(' '.join(permutation))\n",
    "    scores.append((' '.join(permutation),calculate_sentence_score(list(permutation))))\n",
    "    print(i+1,'/',len(permutations))\n",
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "for score in scores:\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4c76b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "mask_filler = pipeline(\"fill-mask\", model=model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2e91e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مدرسه به میروم من\n",
      "1.0030545629170588e-07\n",
      "1 / 24\n",
      "مدرسه به من میروم\n",
      "4.661605925357525e-06\n",
      "2 / 24\n",
      "مدرسه میروم به من\n",
      "4.108130995346479e-05\n",
      "3 / 24\n",
      "مدرسه میروم من به\n",
      "0.00018507368135433228\n",
      "4 / 24\n",
      "مدرسه من به میروم\n",
      "4.7450013952038365e-05\n",
      "5 / 24\n",
      "مدرسه من میروم به\n",
      "4.599659947623106e-06\n",
      "6 / 24\n",
      "به مدرسه میروم من\n",
      "0.00016036466352045958\n",
      "7 / 24\n",
      "به مدرسه من میروم\n",
      "0.0074528035993467155\n",
      "8 / 24\n",
      "به میروم مدرسه من\n",
      "0.04333817848017966\n",
      "9 / 24\n",
      "به میروم من مدرسه\n",
      "0.0013425347248704118\n",
      "10 / 24\n",
      "به من مدرسه میروم\n",
      "0.05005675758318943\n",
      "11 / 24\n",
      "به من میروم مدرسه\n",
      "3.336618776419605e-05\n",
      "12 / 24\n",
      "میروم مدرسه به من\n",
      "0.02359523562444523\n",
      "13 / 24\n",
      "میروم مدرسه من به\n",
      "0.10629790345988384\n",
      "14 / 24\n",
      "میروم به مدرسه من\n",
      "0.015569209748703522\n",
      "15 / 24\n",
      "میروم به من مدرسه\n",
      "0.000482304643606211\n",
      "16 / 24\n",
      "میروم من مدرسه به\n",
      "0.7139499000831163\n",
      "17 / 24\n",
      "میروم من به مدرسه\n",
      "0.004909330053782355\n",
      "18 / 24\n",
      "من مدرسه به میروم\n",
      "0.22939533224943526\n",
      "19 / 24\n",
      "من مدرسه میروم به\n",
      "0.022236885388188497\n",
      "20 / 24\n",
      "من به مدرسه میروم\n",
      "0.1513654747937684\n",
      "21 / 24\n",
      "من به میروم مدرسه\n",
      "0.00010089524565373902\n",
      "22 / 24\n",
      "من میروم مدرسه به\n",
      "6.009466715674936\n",
      "23 / 24\n",
      "من میروم به مدرسه\n",
      "0.04132286530334005\n",
      "24 / 24\n",
      "('من میروم مدرسه به', 6.009466715674936)\n",
      "('میروم من مدرسه به', 0.7139499000831163)\n",
      "('من مدرسه به میروم', 0.22939533224943526)\n",
      "('من به مدرسه میروم', 0.1513654747937684)\n",
      "('میروم مدرسه من به', 0.10629790345988384)\n",
      "('به من مدرسه میروم', 0.05005675758318943)\n",
      "('به میروم مدرسه من', 0.04333817848017966)\n",
      "('من میروم به مدرسه', 0.04132286530334005)\n",
      "('میروم مدرسه به من', 0.02359523562444523)\n",
      "('من مدرسه میروم به', 0.022236885388188497)\n",
      "('میروم به مدرسه من', 0.015569209748703522)\n",
      "('به مدرسه من میروم', 0.0074528035993467155)\n",
      "('میروم من به مدرسه', 0.004909330053782355)\n",
      "('به میروم من مدرسه', 0.0013425347248704118)\n",
      "('میروم به من مدرسه', 0.000482304643606211)\n",
      "('مدرسه میروم من به', 0.00018507368135433228)\n",
      "('به مدرسه میروم من', 0.00016036466352045958)\n",
      "('من به میروم مدرسه', 0.00010089524565373902)\n",
      "('مدرسه من به میروم', 4.7450013952038365e-05)\n",
      "('مدرسه میروم به من', 4.108130995346479e-05)\n",
      "('به من میروم مدرسه', 3.336618776419605e-05)\n",
      "('مدرسه به من میروم', 4.661605925357525e-06)\n",
      "('مدرسه من میروم به', 4.599659947623106e-06)\n",
      "('مدرسه به میروم من', 1.0030545629170588e-07)\n"
     ]
    }
   ],
   "source": [
    "# def calculate_sentence_score(sentence_tokens):\n",
    "\n",
    "#     sentence_score = 1\n",
    "#     for i,token in enumerate(sentence_tokens):\n",
    "#         found = False\n",
    "#         masked_text = ' '.join(tokens[:i])+' [MASK] '+' '.join(tokens[i+1:])\n",
    "#         # print(masked_text)\n",
    "\n",
    "#         # inputs = tokenizer(masked_text, return_tensors=\"tf\")\n",
    "#         # outputs = model(inputs)\n",
    "#         # logits = outputs.logits\n",
    "\n",
    "#         # mask_token_index = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
    "#         # mask_token_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "#         k = 1000\n",
    "#         result = mask_filler(masked_text, top_k=k)\n",
    "        \n",
    "#         # for j in range(len(result)):\n",
    "#         #     print(result[j])\n",
    "#         # print('+++')\n",
    "\n",
    "#         # top_k_values, top_k_indices = tf.math.top_k(mask_token_logits, k=k)\n",
    "#         # predicted_token_indices = top_k_indices.numpy()\n",
    "#         # probabilities = tf.nn.softmax(top_k_values).numpy()\n",
    "#         # predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_indices)\n",
    "\n",
    "#         # for j,predicted_token in enumerate(predicted_tokens):\n",
    "#         #     if predicted_token == token:\n",
    "#         #         sentence_score *= probabilities[j]\n",
    "#         #         # print('found',sentence_score)\n",
    "#         #         found = True\n",
    "#         #         break\n",
    "\n",
    "#         for res in result:\n",
    "#             if res['token_str'] == token:\n",
    "#                 sentence_score *= res['score']\n",
    "#                 # print('found',sentence_score)\n",
    "#                 found = True\n",
    "#                 break\n",
    "\n",
    "#         epsilon = 0.00001\n",
    "\n",
    "#         if not found:\n",
    "#             sentence_score *= epsilon\n",
    "#             # print('not found',sentence_score)\n",
    "\n",
    "\n",
    "#     soft_parameter = 1000000000000\n",
    "\n",
    "#     return sentence_score*soft_parameter\n",
    "\n",
    "\n",
    "# import itertools\n",
    "# text = \"مدرسه به میروم من\"\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "# permutations = list(itertools.permutations(tokens))\n",
    "# scores = []\n",
    "# for i,permutation in enumerate(permutations):\n",
    "#     sentence = ' '.join(permutation)\n",
    "#     score = calculate_sentence_score(list(permutation))\n",
    "#     print(sentence)\n",
    "#     print(score)\n",
    "#     scores.append((sentence,score))\n",
    "#     print(i+1,'/',len(permutations))\n",
    "# scores.sort(key=lambda x: x[1], reverse=True)\n",
    "# for score in scores:\n",
    "#     print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "309acee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m perplexity\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mمن به مدرسه میروم\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 20\u001b[0m sentence_score \u001b[39m=\u001b[39m sentence_perplexity(sentence)\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSentence score: \u001b[39m\u001b[39m\"\u001b[39m, sentence_score)\n",
      "Cell \u001b[1;32mIn[163], line 10\u001b[0m, in \u001b[0;36msentence_perplexity\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      8\u001b[0m tokenized_sentence \u001b[39m=\u001b[39m tokenizer(sentence, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m input_ids \u001b[39m=\u001b[39m tokenized_sentence[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m mask_token_index \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mwhere(input_ids \u001b[39m==\u001b[39;49m tokenizer\u001b[39m.\u001b[39;49mmask_token_id)[\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m]\n\u001b[0;32m     11\u001b[0m output \u001b[39m=\u001b[39m model(tokenized_sentence)\n\u001b[0;32m     12\u001b[0m logits \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7261\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7262\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219829b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
